exp_name: "CIFAR100_ViT_alpha0.1"
dataset:
  data_name: "CIFAR100" # choice: [CIFAR10, CIFAR100, MNIST, FMNIST]
  root_path: "/data0/wxy_data/datasets/"
  train_batch_size: 128
  test_batch_size: 128
  channels: 3
  num_classes: 100
  image_size: 32

  aux_data_name: "CIFAR10"
  aux_root_path: "/data0/wxy_data/datasets/"
  aux_split: 'test'

distribution:
  type: "dirichlet" # choice: [iid, noniid, dirichlet]
  label_num_per_client: 2 
  alpha: 0.1

client:
  num_clients: 10
  
server:
  num_rounds: 50
  frac_clients: 1.0
  lr: 0.001       # Adjusted for ViT (usually prefers lower LR with Adam)
  local_epochs: 5
  optimizer: "adam" # ViT usually works better with Adam
  momentum: 0.9
  weight_decay: 0.0001
  loss_name: "ce"
  model_name: "vit"
  aggregated_by_datasize: True
  lr_decay_per_round: 0.99

device: "cuda:0"

checkpoint:
  save_path: "./checkpoints/"
  save_freq: 10
  result_file: "CIFAR10_ViT_results.yaml"

pretrain: # Not used for 'ours' usually, but keeping structure
  lr: 0.01
  epoch: 0
  model_path: "./pretrain/"
  momentum: 0.9
  weight_decay: 0.0001
  model_name: "vit"
  model_file_name: "dummy.pth"

DBCD:
  alpha_l_pa: 1
  alpha_l_pb: 1
  alpha_l_pc: 0.0
  Beta_alpha: 0.2
  unsup_method: 'infonce'
  contrastive_temperature: 0.5
  encoder_epoch: 40
  projector_epoch: 10
  use_pretrain: False
  aug_batch_size: 128
  encoder_lr: 0.001
  cls_optimizer: 'sgd'
  cls_lr: 0.01
  supcon_temp: 0.07
  super_gr: 100
  aux_with_public: True

etf:
  loss_name: "ce"

visualization:
  interval: 10
  save_path: 'visualization/vit'
  vis_size: 16
resume: False
resume_best: False
seed: 1
